{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4kNqYgK0rhp"
   },
   "source": [
    "# Description\n",
    "\n",
    "The main idea is to test the same CNNs that are known to have a good perfomance in medical image segmentation, particularly those that were tested on a retina dataset such as the Drive dataset. Therefore, we use the following machine learning algorithms to segment the vessels from our egg datasets:\n",
    "\n",
    "- U-net: This model was based in the paper [U-Net: Convolutional Networks for Biomedical](https://arxiv.org/pdf/1505.04597.pdf).\n",
    "\n",
    "- M2V-net: This model was prepared using a pretrained layers of [MobileNetV2](https://arxiv.org/pdf/1801.04381.pdf) as the enconder, and for the decoder it was used a upsample block implemented in pix2pix. The complete description of this model can be found as an example in [Tensorflow site](https://www.tensorflow.org/tutorials/images/segmentation).\n",
    "\n",
    "- M2U-net: This model was based in this [paper](https://arxiv.org/pdf/1811.07738.pdf), some of the layers are based in the MobileNetV2.\n",
    "\n",
    "This project is a work in progress, it is still necessary to get better dataset labels, obfuscate the image background before taking a picture and define a good metric to measure how good the segmentation is. In particular, the metric should be adapted to improve the accuracy of any requested laboratorial measurements.\n",
    "\n",
    "> We tried to use only TensorFlow or Numpy functions, since the model will be faster and easier to deploy using other TensorFlow APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TyewGhwSNI4c"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-781b99d11a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cv2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_registerMatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmat_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import jaccard_score, f1_score, confusion_matrix, \\\n",
    "                            roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUFg2jqPYYvz",
    "outputId": "85f8de9c-4986-41b4-f0da-9d619d4d8a69"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0IdaAX2rLWK"
   },
   "source": [
    "# U-net model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apLtaT4XNmjy"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeLJb_cTYZBt"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 3\n",
    "height = 1532\n",
    "width = 2048\n",
    "\n",
    "# Resize parameters\n",
    "resize_h = height + 4\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slI1TfEWYj_d"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "path = \"./eggs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUs-LYGmYlOW"
   },
   "outputs": [],
   "source": [
    "image_paths = [path + str(i) + '.jpg' for i in range(1, 5)]\n",
    "label_paths= [path + 'bin-seg-' + str(i) + '.jpg' for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy1khziBFSBW",
    "outputId": "a336b1db-3dd2-4399-a022-6b26b1e9a25a"
   },
   "outputs": [],
   "source": [
    "# Number of cuts in an image\n",
    "(height + 4) // crop_wh, width // crop_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgolgJ7bT8O5"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZA4TjgPbocH",
    "outputId": "98e9340b-21ca-436d-b810-a61232387b34"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_h // crop_wh) * (width // crop_wh)\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOkUOH1tOm6d"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    image = tf.io.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    label = tf.io.decode_jpeg(label_string, channels=0)  \n",
    "    label = tf.image.rgb_to_grayscale(label)\n",
    "    \n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5HHKLcRYssV"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    ran = tf.random.uniform([2], minval=1 - 0.3, maxval=1 + 0.3)\n",
    "\n",
    "    image = tf.image.adjust_brightness(image, ran[0])\n",
    "    image = tf.image.adjust_contrast(image, ran[1])\n",
    "    \n",
    "    ran = tf.random.uniform([2], minval=1 - 0.02, maxval=1 + 0.02)\n",
    "\n",
    "    image = tf.image.adjust_saturation(image, ran[0])\n",
    "    image = tf.image.adjust_hue(image, ran[1])\n",
    "\n",
    "    image = rgb2gray(image)\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.py_function(clahe_equalized, [image], tf.uint8)\n",
    "    image.set_shape(image_shape)\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vUZ8TBPQu-2"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ffc3ge7DLuA"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAadxpeKvL9v"
   },
   "outputs": [],
   "source": [
    "def random_flip(image, labels):\n",
    "  \n",
    "  '''does a random flip of the image and mask'''\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    labels = tf.image.flip_left_right(labels)\n",
    "\n",
    "  return image, labels\n",
    "\n",
    "@tf.function\n",
    "def rgb2gray(image):\n",
    "    gray_scale = image[:,:,0]*0.299 + image[:,:,1]*0.587 + image[:,:,2]*0.114\n",
    "    return tf.expand_dims(gray_scale, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrP8digdPTCG"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djvUDRG6Vgtl"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kj3zqasNQ0r5"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3CNpb5F306dO",
    "outputId": "c10a2b3b-9ad1-4a3d-fcc8-af6fa9f17c7a"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzijGId66Mk6"
   },
   "source": [
    "## Define U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4p6TFjfz6Vg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, concatenate, BatchNormalization, \\\n",
    "                                    Conv2D, MaxPooling2D, MaxPool2D, \\\n",
    "                                    UpSampling2D, Reshape, Dropout, Reshape, \\\n",
    "                                    Permute, Activation, Conv2DTranspose,  \\\n",
    "                                    Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rubrluOEeeEj"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_Unet_Eggs.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlam8K47FjFv"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng8_OBu5k2Bo"
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input, n_filters, kernel_size = 3, activation='relu'):\n",
    "\n",
    "  x = input\n",
    "\n",
    "  for i in range(2):\n",
    "    x = Conv2D(filters=n_filters,\n",
    "                               kernel_size=(kernel_size, kernel_size),\n",
    "                               activation=None,\n",
    "                               padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def one_encoder_block(input, n_filters):\n",
    "\n",
    "  f = conv2d_block(input, n_filters)\n",
    "  x = MaxPool2D(pool_size=(2, 2))(f)\n",
    "\n",
    "  return f, x\n",
    "\n",
    "def one_decoder_block(inputs, decoder_output, n_filters, kernel_size = 3):\n",
    "\n",
    "  f = Conv2DTranspose(filters=n_filters,\n",
    "                                      kernel_size=(kernel_size, kernel_size),\n",
    "                                      padding = 'same',\n",
    "                                      strides= (2, 2))(inputs)\n",
    "\n",
    "  f = Concatenate()([f, decoder_output])\n",
    "  f = conv2d_block(f, n_filters, kernel_size=3)\n",
    "\n",
    "  return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6natZyN6UNS"
   },
   "source": [
    "## Build U-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa283mWO6RyJ"
   },
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "\n",
    "  f1, x = one_encoder_block(inputs, 32) \n",
    "  f2, x = one_encoder_block(x, 64) \n",
    "  f3, x = one_encoder_block(x, 128) \n",
    "\n",
    "  return x, (f1, f2, f3)\n",
    "\n",
    "def bottle_neck(input, n_filters=256):\n",
    "  \n",
    "  f = conv2d_block(input, n_filters)\n",
    "\n",
    "  return f\n",
    "\n",
    "def decoder(inputs, encoder_outputs, n_labels):\n",
    "\n",
    "  f1, f2, f3 = encoder_outputs\n",
    "\n",
    "  x = one_decoder_block(inputs, f3, n_filters=128, kernel_size = 3)\n",
    "  x = one_decoder_block(x, f2, n_filters=64, kernel_size = 3)\n",
    "  x = one_decoder_block(x, f1, n_filters=32, kernel_size = 3)\n",
    "\n",
    "  outputs = Conv2D(n_labels, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9ox6dl86PUy"
   },
   "outputs": [],
   "source": [
    "def build_unet(patch_height, patch_width, n_ch):\n",
    "\n",
    "  input = Input(shape=(patch_height, patch_width, n_ch))\n",
    "\n",
    "  x, encoder_outputs = encoder(input)\n",
    "  \n",
    "  bottleneck = bottle_neck(x)\n",
    "\n",
    "  output = decoder(bottleneck, encoder_outputs, 1)\n",
    "\n",
    "  model = Model(inputs=input, outputs=output)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmzd_AcWddxc"
   },
   "outputs": [],
   "source": [
    "model = build_unet(crop_wh, crop_wh, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LltVJjKgeAgg"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEmGq6Yqe86R"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_o9ZpE2exZ3"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9adsA7Rzeylc"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxRpV0y2e_c-"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZrVsdqod9h8"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGyq3M5J6Ith"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74nqtreRcMk-"
   },
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "\n",
    "  jac = (1 - jac)\n",
    "\n",
    "  loss = (tf.keras.losses.binary_crossentropy(y_true, y_pred) +\n",
    "        0.3 * jac)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw6VR901WLuN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=combined_loss, \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdB7CMMRRfj8",
    "outputId": "9dce3670-e514-417f-bd20-5735cd47bd03"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4ySJByZ9mAZ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EIE-1xVGWLrl",
    "outputId": "07b30b64-109a-4910-8834-7c66378da9ee"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=300, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dHBsoskUfzMb",
    "outputId": "792d23a5-5ce9-459d-ae94-7114703bba99"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh7BLGd_8_OR"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHWJyspt9QKU"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBRaKkJJ0Uvl"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_h // crop_wh) * (width // crop_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHhkSG0rp12E"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = rgb2gray(image)\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.py_function(clahe_equalized, [image], tf.uint8)\n",
    "    image.set_shape(image_shape)\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uop6Xgv1Y_fT"
   },
   "outputs": [],
   "source": [
    "test_file_names = [path + str(i) + '.jpg' for i in range(5, 6)]\n",
    "test_labels = [path + 'bin-seg-' + str(i) + '.jpg' for i in range(5, 6)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_file_names, test_labels))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_pmqL8UeeEG",
    "outputId": "16d245d0-8505-49d8-a230-cbd9a5fa328d"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-_5QoZf9aIM"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkQE0Fdgo-Dp"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo4UZt4_23Vs"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_h, width, 1))\n",
    "    label = np.ones((resize_h, width, 1))\n",
    "    image_pred = np.ones((resize_h, width, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjEMmroH346m"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((resize_h, width, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.1)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "bf4wc3nRsHKw",
    "outputId": "374c01ea-ba51-4af9-dece-c38ba738839a"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6-EccdRUEtK"
   },
   "source": [
    "# MobilineNetV2 - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E47E6IqHr3ZM"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEWzEACwr3ZM"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 3\n",
    "height = 1532\n",
    "width = 2048\n",
    "\n",
    "# Resize parameters\n",
    "resize_h = height + 4\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Df9TJTLr3ZM"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "path = \"./eggs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvoTxcMNr3ZM"
   },
   "outputs": [],
   "source": [
    "image_paths = [path + str(i) + '.jpg' for i in range(1, 5)]\n",
    "label_paths= [path + 'bin-seg-' + str(i) + '.jpg' for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oD0bsucr3ZM",
    "outputId": "3fbb594e-dbc5-48a1-bac4-fe25c87a50e2"
   },
   "outputs": [],
   "source": [
    "# Number of cuts in an image\n",
    "(height + 4) // crop_wh, width // crop_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB3xbLhYqozW"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7O4ypZHqozW",
    "outputId": "b68cd38d-7aac-4881-edfc-878c093f6bc8"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_h // crop_wh) * (width // crop_wh)\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaXhZLeTqozX"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    image = tf.io.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    label = tf.io.decode_jpeg(label_string, channels=0)  \n",
    "    label = tf.image.rgb_to_grayscale(label)\n",
    "    \n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHW1Si5RqozX"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    ran = tf.random.uniform([2], minval=1 - 0.3, maxval=1 + 0.3)\n",
    "\n",
    "    image = tf.image.adjust_brightness(image, ran[0])\n",
    "    image = tf.image.adjust_contrast(image, ran[1])\n",
    "    \n",
    "    ran = tf.random.uniform([2], minval=1 - 0.02, maxval=1 + 0.02)\n",
    "\n",
    "    image = tf.image.adjust_saturation(image, ran[0])\n",
    "    image = tf.image.adjust_hue(image, ran[1])\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    #image, label = random_flip(image, label)\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZzpBcr7qozX"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Vp2HicaqozX"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UteVrHWRqozX"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvQwhcx3qozX"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-U_FhrsqozY"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExKhldSXqozY",
    "outputId": "01434248-8a39-4496-d018-ddb9b686fd8e"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2Rf3zpMuuDF"
   },
   "source": [
    "## Define M2V-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmjNUzqIuuDG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, concatenate, BatchNormalization, \\\n",
    "                                    Conv2D, MaxPooling2D, MaxPool2D, \\\n",
    "                                    UpSampling2D, Reshape, Dropout, Reshape, \\\n",
    "                                    Permute, Activation, Conv2DTranspose,  \\\n",
    "                                    Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ8QrZi8uuDG"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_M2Vnet_Eggs.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCWlgF1ZvNBI",
    "outputId": "778e9454-9872-4e30-c148-aee14fe6a7d1"
   },
   "outputs": [],
   "source": [
    "pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqW-vto1uuDH"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBRSn5ORu81g"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[crop_wh, crop_wh, 3], include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AE-vHTxTuuDH"
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Crie o modelo de extração de características\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEMsAsEhqkzd"
   },
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-VbZAS2v3eO"
   },
   "source": [
    "## Build M2V-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZiWNRnfUFqO"
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\n",
    "\n",
    "  # Esta é a última camada do modelo\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same', activation='sigmoid')  #64x64 -> 128x128\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling através do modelo\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling e estabelecimento das conexões de salto\n",
    "  \n",
    "  for i, (up, skip) in enumerate(zip(up_stack, skips)):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  \n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWBeuZJLWU_Y"
   },
   "outputs": [],
   "source": [
    "model = unet_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erz3P91-wKht"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p7dmcL1wKht"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TPOFb8gwKht"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aky4nRpZwKhu"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxLXhFJtwKhu"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8FRwT-8wKhu"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-amCh2HwUML"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg6gqwU1FW4l"
   },
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "\n",
    "  jac = (1 - jac)\n",
    "\n",
    "  loss = (tf.keras.losses.binary_crossentropy(y_true, y_pred) +\n",
    "        0.3 * jac)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXpdlA1ewUMM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=combined_loss, \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alg4EELywUMN",
    "outputId": "83ea0363-e5a0-4089-bbcc-b1e709d7ed4b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lcgo3devwnYu"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kEar8wKNwnYv",
    "outputId": "20b09546-3d2b-48c2-e342-8fba999bb847"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=300, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YnGJycLmwnYw",
    "outputId": "55b7bd9f-7583-4e3b-e3ac-d2d6b34a7b0d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNETOgEywQa"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqv4hfmrywQa"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5txTmM4sywQa"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_h // crop_wh) * (width // crop_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq99lD_CywQa"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dMvBFKBywQa"
   },
   "outputs": [],
   "source": [
    "test_file_names = [path + str(i) + '.jpg' for i in range(5, 6)]\n",
    "test_labels = [path + 'bin-seg-' + str(i) + '.jpg' for i in range(5, 6)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_file_names, test_labels))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfCDf4T1ywQb",
    "outputId": "1f5a9557-020f-4358-bdb4-2325d7fe0fc2"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToujTJ4TywQb"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_IVGK-UywQb"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiz6o46pywQb"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_h, width, 3))\n",
    "    label = np.ones((resize_h, width, 1))\n",
    "    image_pred = np.ones((resize_h, width, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm1Y_aeqywQb"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((resize_h, width, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.1)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "CZiPinl6ywQb",
    "outputId": "85299d45-dc31-42dd-c884-f977abaf6721"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxRJOUd3DnlG"
   },
   "source": [
    "# M2U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkr6HKTrKeVg"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nurRm8HzKeVh"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 3\n",
    "height = 1532\n",
    "width = 2048\n",
    "\n",
    "# Resize parameters\n",
    "resize_h = height + 4\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuGfu3-mKeVh"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "path = \"./eggs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Py1hUivKeVh"
   },
   "outputs": [],
   "source": [
    "image_paths = [path + str(i) + '.jpg' for i in range(1, 5)]\n",
    "label_paths= [path + 'bin-seg-' + str(i) + '.jpg' for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCdIbBAkKeVh",
    "outputId": "a4d8440c-5815-440d-cc41-52fe65b7326d"
   },
   "outputs": [],
   "source": [
    "# Number of cuts in an image\n",
    "(height + 4) // crop_wh, width // crop_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-KT3WuXOD2k"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSAMF7-FOD2k",
    "outputId": "ee53d006-8714-4b4b-a437-970b36186eef"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_h // crop_wh) * (width // crop_wh)\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-faJYb2OD2l"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    image = tf.io.decode_jpeg(image_string, channels=3)\n",
    "    \n",
    "    label = tf.io.decode_jpeg(label_string, channels=0)  \n",
    "    label = tf.image.rgb_to_grayscale(label)\n",
    "    \n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZA2Gt_UcOD2l"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    ran = tf.random.uniform([2], minval=1 - 0.3, maxval=1 + 0.3)\n",
    "\n",
    "    image = tf.image.adjust_brightness(image, ran[0])\n",
    "    image = tf.image.adjust_contrast(image, ran[1])\n",
    "    \n",
    "    ran = tf.random.uniform([2], minval=1 - 0.02, maxval=1 + 0.02)\n",
    "\n",
    "    image = tf.image.adjust_saturation(image, ran[0])\n",
    "    image = tf.image.adjust_hue(image, ran[1])\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    #image, label = random_flip(image, label)\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQGp4nDdOD2l"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed, interpolation='nearest')\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG9rOrthOD2l"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuMen6x3OD2l"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI5kpiHMOD2l"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dY8GCctiOD2l"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czHz-qijOD2m",
    "outputId": "378d045a-3d97-417b-b07c-b89ecbcfbab4"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi75MldIZAB_"
   },
   "source": [
    "## Define M2U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4cQSqGWZHhd"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_M2Unet_Egg.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_9c90C7ZQtV"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esJWuNfNDqxY"
   },
   "outputs": [],
   "source": [
    "def DepthwiseConv(input, channels, stride=1):\n",
    "\n",
    "  x = tf.keras.layers.DepthwiseConv2D(kernel_size=3,\n",
    "                                      strides=stride,\n",
    "                                      padding='same')(input)\n",
    "\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)                                    \n",
    "  x = tf.keras.layers.Conv2D(channels, 1, strides=1, padding='same')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t4yOn3RInUn"
   },
   "outputs": [],
   "source": [
    "def bottleneck_block(input, input_channels, output_channels, stride, factor):\n",
    "  \n",
    "  x = tf.keras.layers.Conv2D(round(input_channels * factor), 1, strides=1, padding='same')(input)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)  \n",
    "  x = DepthwiseConv(x, output_channels, stride)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OWAWkzEMGBO"
   },
   "outputs": [],
   "source": [
    "def res_bottle_neck(input, channels, factor):\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(round(channels * factor), 1, strides=1, padding='same')(input)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)\n",
    "  x = DepthwiseConv(x, channels, 1)\n",
    "  x = tf.keras.layers.Add()([x, input])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqnWQYzNRI9s"
   },
   "outputs": [],
   "source": [
    "def upconcat(input, skip_input):\n",
    "\n",
    "  x = tf.keras.layers.UpSampling2D(interpolation='bilinear')(input)\n",
    "  x = tf.concat([x, skip_input], axis=-1)  \n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCo1PYo3ZW7A"
   },
   "source": [
    "## Build M2U-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjU-HP5lDqvJ"
   },
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(input)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.activations.relu(x)\n",
    "\n",
    "skip1 = DepthwiseConv(x, channels=16, stride=1)\n",
    "\n",
    "x = bottleneck_block(skip1, 16, 24, 2, 6)\n",
    "skip2 = res_bottle_neck(x, 24, 6)\n",
    "\n",
    "x = bottleneck_block(skip2, 24, 32, 2, 6)\n",
    "x = res_bottle_neck(x, 32, 6)\n",
    "skip3 = res_bottle_neck(x, 32, 6)\n",
    "\n",
    "x = bottleneck_block(skip3, 32, 64, 2, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = bottleneck_block(x, 64, 96, 1, 6)\n",
    "\n",
    "x = res_bottle_neck(x, 96, 6)\n",
    "x = res_bottle_neck(x, 96, 6)\n",
    "\n",
    "x = upconcat(x, skip3)\n",
    "x = bottleneck_block(x, 128, 64, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, skip2)\n",
    "x = bottleneck_block(x, 88, 44, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, skip1)\n",
    "x = bottleneck_block(x, 60, 30, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, input)\n",
    "x = bottleneck_block(x, 30, 1, 1, 0.15)\n",
    "\n",
    "x = tf.keras.activations.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iaBO2z3o0Ge"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec_dY-MpZfgF"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNPELNlcZfgF"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF_zd_nTZfgG"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy311ShvZfgG"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We1r6hnsZfgH"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnyaCeliZfgH"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S66SngtwZljs"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbE-8JKgZljt"
   },
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "\n",
    "  jac = (1 - jac)\n",
    "\n",
    "  loss = (tf.keras.losses.binary_crossentropy(y_true, y_pred) +\n",
    "        0.3 * jac)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqJHcAiOZlju"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=combined_loss, \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6vjaO-GZlju",
    "outputId": "40c92597-5cd5-4963-f583-18c7180700dd"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9xlGucUZvWF"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7jPUCW59ZvWF",
    "outputId": "6d1cd8bd-ae6f-4b73-f155-cbdbcb8f531d"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=300, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v8GkYAeiZvWG",
    "outputId": "b98d1bdb-4d93-47ef-8ea0-e30981410b97"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apTWYsL4K4W_"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIYNt_RSK4W_"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtJgRqN9K4W_"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_h // crop_wh) * (width // crop_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfik2d5DK4XA"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.image.resize(image, [resize_h, width], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_h, width], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ci8EBjoVK4XA"
   },
   "outputs": [],
   "source": [
    "test_file_names = [path + str(i) + '.jpg' for i in range(5, 6)]\n",
    "test_labels = [path + 'bin-seg-' + str(i) + '.jpg' for i in range(5, 6)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_file_names, test_labels))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5MQ1T7cK4XA",
    "outputId": "3a6cf79c-e42d-447c-afc5-dfc9257a7309"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVrlVEAKK4XA"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-laZsM89K4XA"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM9kyP3qK4XA"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_h, width, 3))\n",
    "    label = np.ones((resize_h, width, 1))\n",
    "    image_pred = np.ones((resize_h, width, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b25dcXwdK4XA"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((resize_h, width, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.1)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "g6efJnSSK4XA",
    "outputId": "e7912110-13a8-4e96-a0a3-583156ec0f4d"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88b90xndqy52"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e0IdaAX2rLWK",
    "apLtaT4XNmjy",
    "XgolgJ7bT8O5",
    "HzijGId66Mk6",
    "Vlam8K47FjFv",
    "S6natZyN6UNS",
    "LltVJjKgeAgg",
    "aEmGq6Yqe86R",
    "fxRpV0y2e_c-",
    "LGyq3M5J6Ith",
    "K4ySJByZ9mAZ",
    "Nh7BLGd_8_OR",
    "xHWJyspt9QKU",
    "L-_5QoZf9aIM",
    "y6-EccdRUEtK",
    "E47E6IqHr3ZM",
    "RB3xbLhYqozW",
    "a2Rf3zpMuuDF",
    "PqW-vto1uuDH",
    "4-VbZAS2v3eO",
    "Erz3P91-wKht",
    "2p7dmcL1wKht",
    "sxLXhFJtwKhu",
    "s-amCh2HwUML",
    "Lcgo3devwnYu",
    "GGNETOgEywQa",
    "aqv4hfmrywQa",
    "ToujTJ4TywQb",
    "QxRJOUd3DnlG",
    "Rkr6HKTrKeVg",
    "i-KT3WuXOD2k",
    "zi75MldIZAB_",
    "T_9c90C7ZQtV",
    "wCo1PYo3ZW7A",
    "ec_dY-MpZfgF",
    "HNPELNlcZfgF",
    "We1r6hnsZfgH",
    "S66SngtwZljs",
    "R9xlGucUZvWF",
    "apTWYsL4K4W_",
    "RIYNt_RSK4W_",
    "TVrlVEAKK4XA"
   ],
   "name": "(Eggs) U-net Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
