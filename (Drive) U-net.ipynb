{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "AWJzZzHkdWYs",
   "metadata": {
    "id": "AWJzZzHkdWYs"
   },
   "source": [
    "# Description\n",
    "\n",
    "The main idea is to test the same CNNs that are known to have a good perfomance in medical image segmentation, particularly those that were tested on a retina dataset such as the Drive dataset. Therefore, to get some experience we Three models were tested using the [DRIVE dataset](https://drive.grand-challenge.org/).\n",
    "\n",
    "- U-net: This model was based in the paper [U-Net: Convolutional Networks for Biomedical](https://arxiv.org/pdf/1505.04597.pdf).\n",
    "\n",
    "- M2V-net: This model was prepared using a pretrained layers of [MobileNetV2](https://arxiv.org/pdf/1801.04381.pdf) as the enconder, and for the decoder it was used a upsample block implemented in pix2pix. The complete description of this model can be found as an example in [Tensorflow site](https://www.tensorflow.org/tutorials/images/segmentation).\n",
    "\n",
    "- M2U-net: This model was based in this [paper](https://arxiv.org/pdf/1811.07738.pdf), some of the layers are based in the MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TyewGhwSNI4c",
   "metadata": {
    "id": "TyewGhwSNI4c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.metrics import jaccard_score, f1_score, confusion_matrix, \\\n",
    "                            roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vUFg2jqPYYvz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUFg2jqPYYvz",
    "outputId": "78e581b4-7266-46f8-cb1e-60ebb18adbcc"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0IdaAX2rLWK",
   "metadata": {
    "id": "e0IdaAX2rLWK"
   },
   "source": [
    "# U-net model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apLtaT4XNmjy",
   "metadata": {
    "id": "apLtaT4XNmjy"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yeLJb_cTYZBt",
   "metadata": {
    "id": "yeLJb_cTYZBt"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 4\n",
    "height = 584\n",
    "width = 565\n",
    "\n",
    "# Resize parameters\n",
    "resize_wh = 512\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "slI1TfEWYj_d",
   "metadata": {
    "id": "slI1TfEWYj_d"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "imgs_train = \"./DRIVE/training/images/\"\n",
    "label_imgs_train = \"./DRIVE/training/1st_manual/\"\n",
    "masks_train = \"./DRIVE/training/mask/\"\n",
    "\n",
    "imgs_test = \"./DRIVE/test/images/\"\n",
    "label_imgs_test = \"./DRIVE/test/1st_manual/\"\n",
    "masks_test = \"./DRIVE/test/mask/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUs-LYGmYlOW",
   "metadata": {
    "id": "IUs-LYGmYlOW"
   },
   "outputs": [],
   "source": [
    "image_paths = [imgs_train + str(i) + '_training.tif' for i in range(21, 41)]\n",
    "label_paths = [label_imgs_train + str(i) + '_manual1.gif' for i in range(21, 41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XgolgJ7bT8O5",
   "metadata": {
    "id": "XgolgJ7bT8O5"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ZA4TjgPbocH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZA4TjgPbocH",
    "outputId": "41b72847-6ee9-465a-c7be-bb69d617aa5a"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_wh // crop_wh)**2\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AOkUOH1tOm6d",
   "metadata": {
    "id": "AOkUOH1tOm6d"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tfio.experimental.image.decode_tiff(image_string)\n",
    "\n",
    "    label = tf.image.decode_gif(label_string)\n",
    "    label = tf.squeeze(tf.image.rgb_to_grayscale(label), axis=0)\n",
    "    label_shape = label.shape    \n",
    "\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z5HHKLcRYssV",
   "metadata": {
    "id": "z5HHKLcRYssV"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    image = rgb2gray(image)\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image, label = normalize(image, label)\n",
    "\n",
    "    image = tf.py_function(clahe_equalized, [image], tf.uint8)\n",
    "    image.set_shape(image_shape)\n",
    "\n",
    "    image = tf.image.adjust_gamma(image, gamma=1.2)\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    #image, label = random_flip(image, label)\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8vUZ8TBPQu-2",
   "metadata": {
    "id": "8vUZ8TBPQu-2"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Ffc3ge7DLuA",
   "metadata": {
    "id": "0Ffc3ge7DLuA"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QAadxpeKvL9v",
   "metadata": {
    "id": "QAadxpeKvL9v"
   },
   "outputs": [],
   "source": [
    "def normalize(image, labels):\n",
    "  \n",
    "  mean = 83.15841582964605\n",
    "  std = 57.07514784246855 \n",
    "\n",
    "  image = (image - mean) / std\n",
    "\n",
    "  max = tf.math.reduce_max(image)\n",
    "  min = tf.math.reduce_min(image)\n",
    "\n",
    "  image = 255 * (image - min) / (max - min)\n",
    "\n",
    "  return image, labels\n",
    "\n",
    "def random_flip(image, labels):\n",
    "  \n",
    "  '''does a random flip of the image and mask'''\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    labels = tf.image.flip_left_right(labels)\n",
    "\n",
    "  return image, labels\n",
    "\n",
    "def rgb2gray(image):\n",
    "    gray_scale = image[:,:,0]*0.299 + image[:,:,1]*0.587 + image[:,:,2]*0.114\n",
    "    return tf.expand_dims(gray_scale, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qrP8digdPTCG",
   "metadata": {
    "id": "qrP8digdPTCG"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "djvUDRG6Vgtl",
   "metadata": {
    "id": "djvUDRG6Vgtl"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kj3zqasNQ0r5",
   "metadata": {
    "id": "kj3zqasNQ0r5"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3CNpb5F306dO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3CNpb5F306dO",
    "outputId": "8eb29fb9-0d2c-411c-fcd7-46f2b940cc1c"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HzijGId66Mk6",
   "metadata": {
    "id": "HzijGId66Mk6"
   },
   "source": [
    "## Define U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E4p6TFjfz6Vg",
   "metadata": {
    "id": "E4p6TFjfz6Vg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, concatenate, BatchNormalization, \\\n",
    "                                    Conv2D, MaxPooling2D, MaxPool2D, \\\n",
    "                                    UpSampling2D, Reshape, Dropout, Reshape, \\\n",
    "                                    Permute, Activation, Conv2DTranspose,  \\\n",
    "                                    Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubrluOEeeEj",
   "metadata": {
    "id": "rubrluOEeeEj"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_Unet_DRIVE.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vlam8K47FjFv",
   "metadata": {
    "id": "Vlam8K47FjFv"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ng8_OBu5k2Bo",
   "metadata": {
    "id": "Ng8_OBu5k2Bo"
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input, n_filters, kernel_size = 3, activation='relu'):\n",
    "\n",
    "  x = input\n",
    "\n",
    "  for i in range(2):\n",
    "    x = Conv2D(filters=n_filters,\n",
    "                               kernel_size=(kernel_size, kernel_size),\n",
    "                               activation=None,\n",
    "                               padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "def one_encoder_block(input, n_filters):\n",
    "\n",
    "  f = conv2d_block(input, n_filters)\n",
    "  x = MaxPool2D(pool_size=(2, 2))(f)\n",
    "\n",
    "  return f, x\n",
    "\n",
    "def one_decoder_block(inputs, decoder_output, n_filters, kernel_size = 3):\n",
    "\n",
    "  f = Conv2DTranspose(filters=n_filters,\n",
    "                                      kernel_size=(kernel_size, kernel_size),\n",
    "                                      padding = 'same',\n",
    "                                      strides= (2, 2))(inputs)\n",
    "\n",
    "  f = Concatenate()([f, decoder_output])\n",
    "  f = conv2d_block(f, n_filters, kernel_size=3)\n",
    "\n",
    "  return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S6natZyN6UNS",
   "metadata": {
    "id": "S6natZyN6UNS"
   },
   "source": [
    "## Build U-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pa283mWO6RyJ",
   "metadata": {
    "id": "Pa283mWO6RyJ"
   },
   "outputs": [],
   "source": [
    "def encoder(inputs):\n",
    "\n",
    "  f1, x = one_encoder_block(inputs, 32) \n",
    "  f2, x = one_encoder_block(x, 64) \n",
    "  f3, x = one_encoder_block(x, 128) \n",
    "\n",
    "  return x, (f1, f2, f3)\n",
    "\n",
    "def bottle_neck(input, n_filters=256):\n",
    "  \n",
    "  f = conv2d_block(input, n_filters)\n",
    "\n",
    "  return f\n",
    "\n",
    "def decoder(inputs, encoder_outputs, n_labels):\n",
    "\n",
    "  f1, f2, f3 = encoder_outputs\n",
    "\n",
    "  x = one_decoder_block(inputs, f3, n_filters=128, kernel_size = 3)\n",
    "  x = one_decoder_block(x, f2, n_filters=64, kernel_size = 3)\n",
    "  x = one_decoder_block(x, f1, n_filters=32, kernel_size = 3)\n",
    "\n",
    "  outputs = Conv2D(n_labels, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D9ox6dl86PUy",
   "metadata": {
    "id": "D9ox6dl86PUy"
   },
   "outputs": [],
   "source": [
    "def build_unet(patch_height, patch_width, n_ch):\n",
    "\n",
    "  input = Input(shape=(patch_height, patch_width, n_ch))\n",
    "\n",
    "  x, encoder_outputs = encoder(input)\n",
    "  \n",
    "  bottleneck = bottle_neck(x)\n",
    "\n",
    "  output = decoder(bottleneck, encoder_outputs, 1)\n",
    "\n",
    "  model = Model(inputs=input, outputs=output)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zmzd_AcWddxc",
   "metadata": {
    "id": "Zmzd_AcWddxc"
   },
   "outputs": [],
   "source": [
    "model = build_unet(crop_wh, crop_wh, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LltVJjKgeAgg",
   "metadata": {
    "id": "LltVJjKgeAgg"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEmGq6Yqe86R",
   "metadata": {
    "id": "aEmGq6Yqe86R"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_o9ZpE2exZ3",
   "metadata": {
    "id": "e_o9ZpE2exZ3"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adsA7Rzeylc",
   "metadata": {
    "id": "9adsA7Rzeylc"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxRpV0y2e_c-",
   "metadata": {
    "id": "fxRpV0y2e_c-"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EZrVsdqod9h8",
   "metadata": {
    "id": "EZrVsdqod9h8"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LGyq3M5J6Ith",
   "metadata": {
    "id": "LGyq3M5J6Ith"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yw6VR901WLuN",
   "metadata": {
    "id": "Yw6VR901WLuN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XdB7CMMRRfj8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdB7CMMRRfj8",
    "outputId": "0e235904-8f70-47d3-d414-38888b037823"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K4ySJByZ9mAZ",
   "metadata": {
    "id": "K4ySJByZ9mAZ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EIE-1xVGWLrl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EIE-1xVGWLrl",
    "outputId": "44571370-01c5-4061-8a9b-e2e0aa7bf9ca"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=150, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dHBsoskUfzMb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dHBsoskUfzMb",
    "outputId": "f7eba272-aa2d-420b-b027-b858500569a5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nh7BLGd_8_OR",
   "metadata": {
    "id": "Nh7BLGd_8_OR"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xHWJyspt9QKU",
   "metadata": {
    "id": "xHWJyspt9QKU"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xBRaKkJJ0Uvl",
   "metadata": {
    "id": "xBRaKkJJ0Uvl"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_wh // crop_wh)**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HHhkSG0rp12E",
   "metadata": {
    "id": "HHhkSG0rp12E"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = rgb2gray(image)\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image, label = normalize(image, label)\n",
    "\n",
    "    image = tf.py_function(clahe_equalized, [image], tf.uint8)\n",
    "    image.set_shape(image_shape)\n",
    "\n",
    "    image = tf.image.adjust_gamma(image, gamma=1.2)\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uop6Xgv1Y_fT",
   "metadata": {
    "id": "Uop6Xgv1Y_fT"
   },
   "outputs": [],
   "source": [
    "test_image_paths = [imgs_test + str(i).zfill(2) + '_test.tif' for i in range(1, 21)]\n",
    "test_label_paths = [label_imgs_test + str(i).zfill(2) + '_manual1.gif' for i in range(1, 21)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_label_paths))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B_pmqL8UeeEG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_pmqL8UeeEG",
    "outputId": "1c823e64-02b6-40a3-bbf7-c05a44a251ff"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L-_5QoZf9aIM",
   "metadata": {
    "id": "L-_5QoZf9aIM"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DkQE0Fdgo-Dp",
   "metadata": {
    "id": "DkQE0Fdgo-Dp"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zo4UZt4_23Vs",
   "metadata": {
    "id": "Zo4UZt4_23Vs"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_wh, resize_wh, 1))\n",
    "    label = np.ones((resize_wh, resize_wh, 1))\n",
    "    image_pred = np.ones((resize_wh, resize_wh, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjEMmroH346m",
   "metadata": {
    "id": "yjEMmroH346m"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((512, 512, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.0)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4wc3nRsHKw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bf4wc3nRsHKw",
    "outputId": "927be2bf-1f30-4757-cecf-03dc81c1d0df"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6-EccdRUEtK",
   "metadata": {
    "id": "y6-EccdRUEtK"
   },
   "source": [
    "# MobilineNetV2 - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d5XNArZne",
   "metadata": {
    "id": "bc8d5XNArZne"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jdhTYlYbrZnf",
   "metadata": {
    "id": "jdhTYlYbrZnf"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 4\n",
    "height = 584\n",
    "width = 565\n",
    "\n",
    "# Resize parameters\n",
    "resize_wh = 512\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VTir6mMBrZnf",
   "metadata": {
    "id": "VTir6mMBrZnf"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "imgs_train = \"./DRIVE/training/images/\"\n",
    "label_imgs_train = \"./DRIVE/training/1st_manual/\"\n",
    "masks_train = \"./DRIVE/training/mask/\"\n",
    "\n",
    "imgs_test = \"./DRIVE/test/images/\"\n",
    "label_imgs_test = \"./DRIVE/test/1st_manual/\"\n",
    "masks_test = \"./DRIVE/test/mask/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kyqqfENUrZnf",
   "metadata": {
    "id": "kyqqfENUrZnf"
   },
   "outputs": [],
   "source": [
    "image_paths = [imgs_train + str(i) + '_training.tif' for i in range(21, 41)]\n",
    "label_paths = [label_imgs_train + str(i) + '_manual1.gif' for i in range(21, 41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RB3xbLhYqozW",
   "metadata": {
    "id": "RB3xbLhYqozW"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7O4ypZHqozW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7O4ypZHqozW",
    "outputId": "864fa6a1-b737-4ffc-9cc8-1c228bb9f2da"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_wh // crop_wh)**2\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZaXhZLeTqozX",
   "metadata": {
    "id": "ZaXhZLeTqozX"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tfio.experimental.image.decode_tiff(image_string)\n",
    "\n",
    "    label = tf.image.decode_gif(label_string)\n",
    "    label = tf.squeeze(tf.image.rgb_to_grayscale(label), axis=0)\n",
    "    label_shape = label.shape    \n",
    "\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xHW1Si5RqozX",
   "metadata": {
    "id": "xHW1Si5RqozX"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    ran = tf.random.uniform([2], minval=1 - 0.3, maxval=1 + 0.3)\n",
    "\n",
    "    image = tf.image.adjust_brightness(image, ran[0])\n",
    "    image = tf.image.adjust_contrast(image, ran[1])\n",
    "    \n",
    "    ran = tf.random.uniform([2], minval=1 - 0.02, maxval=1 + 0.02)\n",
    "\n",
    "    image = tf.image.adjust_saturation(image, ran[0])\n",
    "    image = tf.image.adjust_hue(image, ran[1])\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    #image, label = random_flip(image, label)\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wZzpBcr7qozX",
   "metadata": {
    "id": "wZzpBcr7qozX"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5Vp2HicaqozX",
   "metadata": {
    "id": "5Vp2HicaqozX"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uo7cVuLRqozX",
   "metadata": {
    "id": "uo7cVuLRqozX"
   },
   "outputs": [],
   "source": [
    "def normalize(image, labels):\n",
    "  \n",
    "  mean = 83.15841582964605\n",
    "  std = 57.07514784246855 \n",
    "\n",
    "  image = (image - mean) / std\n",
    "\n",
    "  max = tf.math.reduce_max(image)\n",
    "  min = tf.math.reduce_min(image)\n",
    "\n",
    "  image = 255 * (image - min) / (max - min)\n",
    "\n",
    "  return image, labels\n",
    "\n",
    "def random_flip(image, labels):\n",
    "  \n",
    "  '''does a random flip of the image and mask'''\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    labels = tf.image.flip_left_right(labels)\n",
    "\n",
    "  return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UteVrHWRqozX",
   "metadata": {
    "id": "UteVrHWRqozX"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LvQwhcx3qozX",
   "metadata": {
    "id": "LvQwhcx3qozX"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-U_FhrsqozY",
   "metadata": {
    "id": "7-U_FhrsqozY"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ExKhldSXqozY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExKhldSXqozY",
    "outputId": "c5fae944-d36c-40bf-98d3-228fd58c2fcf"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2Rf3zpMuuDF",
   "metadata": {
    "id": "a2Rf3zpMuuDF"
   },
   "source": [
    "## Define M2V-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmjNUzqIuuDG",
   "metadata": {
    "id": "mmjNUzqIuuDG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, concatenate, BatchNormalization, \\\n",
    "                                    Conv2D, MaxPooling2D, MaxPool2D, \\\n",
    "                                    UpSampling2D, Reshape, Dropout, Reshape, \\\n",
    "                                    Permute, Activation, Conv2DTranspose,  \\\n",
    "                                    Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZ8QrZi8uuDG",
   "metadata": {
    "id": "nZ8QrZi8uuDG"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_M2Vnet_DRIVE.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mCWlgF1ZvNBI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCWlgF1ZvNBI",
    "outputId": "a094990e-1cc0-4067-f050-dbc52fba9d34"
   },
   "outputs": [],
   "source": [
    "pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PqW-vto1uuDH",
   "metadata": {
    "id": "PqW-vto1uuDH"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nBRSn5ORu81g",
   "metadata": {
    "id": "nBRSn5ORu81g"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[crop_wh, crop_wh, 3], include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AE-vHTxTuuDH",
   "metadata": {
    "id": "AE-vHTxTuuDH"
   },
   "outputs": [],
   "source": [
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Crie o modelo de extração de características\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oEMsAsEhqkzd",
   "metadata": {
    "id": "oEMsAsEhqkzd"
   },
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4-VbZAS2v3eO",
   "metadata": {
    "id": "4-VbZAS2v3eO"
   },
   "source": [
    "## Build M2V-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZiWNRnfUFqO",
   "metadata": {
    "id": "RZiWNRnfUFqO"
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\n",
    "\n",
    "  # Esta é a última camada do modelo\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same', activation='sigmoid')  #64x64 -> 128x128\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling através do modelo\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling e estabelecimento das conexões de salto\n",
    "  \n",
    "  for i, (up, skip) in enumerate(zip(up_stack, skips)):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  \n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gWBeuZJLWU_Y",
   "metadata": {
    "id": "gWBeuZJLWU_Y"
   },
   "outputs": [],
   "source": [
    "model = unet_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Erz3P91-wKht",
   "metadata": {
    "id": "Erz3P91-wKht"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2p7dmcL1wKht",
   "metadata": {
    "id": "2p7dmcL1wKht"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1TPOFb8gwKht",
   "metadata": {
    "id": "1TPOFb8gwKht"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aky4nRpZwKhu",
   "metadata": {
    "id": "aky4nRpZwKhu"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxLXhFJtwKhu",
   "metadata": {
    "id": "sxLXhFJtwKhu"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z8FRwT-8wKhu",
   "metadata": {
    "id": "Z8FRwT-8wKhu"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s-amCh2HwUML",
   "metadata": {
    "id": "s-amCh2HwUML"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kg6gqwU1FW4l",
   "metadata": {
    "id": "Kg6gqwU1FW4l"
   },
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "\n",
    "  jac = (1 - jac)\n",
    "\n",
    "  loss = (tf.keras.losses.binary_crossentropy(y_true, y_pred) +\n",
    "        0.3 * jac)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CXpdlA1ewUMM",
   "metadata": {
    "id": "CXpdlA1ewUMM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=combined_loss, \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alg4EELywUMN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alg4EELywUMN",
    "outputId": "c527b26c-c4e8-4042-8331-36c71532e81b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lcgo3devwnYu",
   "metadata": {
    "id": "Lcgo3devwnYu"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kEar8wKNwnYv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kEar8wKNwnYv",
    "outputId": "4d71aa65-553f-4117-c6d6-963317f13d4a"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=150, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YnGJycLmwnYw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YnGJycLmwnYw",
    "outputId": "75d8bcb1-966d-4f18-b6b7-cd73473a1a7d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGNETOgEywQa",
   "metadata": {
    "id": "GGNETOgEywQa"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqv4hfmrywQa",
   "metadata": {
    "id": "aqv4hfmrywQa"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5txTmM4sywQa",
   "metadata": {
    "id": "5txTmM4sywQa"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_wh // crop_wh)**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rq99lD_CywQa",
   "metadata": {
    "id": "rq99lD_CywQa"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dMvBFKBywQa",
   "metadata": {
    "id": "4dMvBFKBywQa"
   },
   "outputs": [],
   "source": [
    "test_image_paths = [imgs_test + str(i).zfill(2) + '_test.tif' for i in range(1, 21)]\n",
    "test_label_paths = [label_imgs_test + str(i).zfill(2) + '_manual1.gif' for i in range(1, 21)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_label_paths))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TfCDf4T1ywQb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfCDf4T1ywQb",
    "outputId": "0364cff0-f013-4c82-ee16-33031d6c756d"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ToujTJ4TywQb",
   "metadata": {
    "id": "ToujTJ4TywQb"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U_IVGK-UywQb",
   "metadata": {
    "id": "U_IVGK-UywQb"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uiz6o46pywQb",
   "metadata": {
    "id": "uiz6o46pywQb"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_wh, resize_wh, 3))\n",
    "    label = np.ones((resize_wh, resize_wh, 1))\n",
    "    image_pred = np.ones((resize_wh, resize_wh, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pm1Y_aeqywQb",
   "metadata": {
    "id": "pm1Y_aeqywQb"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((512, 512, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.0)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CZiPinl6ywQb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CZiPinl6ywQb",
    "outputId": "f6a5cdd3-fbe7-4c15-bbe8-6d5b34586699"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QxRJOUd3DnlG",
   "metadata": {
    "id": "QxRJOUd3DnlG"
   },
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2yrKfnnQYQu5",
   "metadata": {
    "id": "2yrKfnnQYQu5"
   },
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1j0UjKhqYQu6",
   "metadata": {
    "id": "1j0UjKhqYQu6"
   },
   "outputs": [],
   "source": [
    "# Images parameters\n",
    "channels = 4\n",
    "height = 584\n",
    "width = 565\n",
    "\n",
    "# Resize parameters\n",
    "resize_wh = 512\n",
    "crop_wh = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CS8NGs07YQu7",
   "metadata": {
    "id": "CS8NGs07YQu7"
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "imgs_train = \"./DRIVE/training/images/\"\n",
    "label_imgs_train = \"./DRIVE/training/1st_manual/\"\n",
    "masks_train = \"./DRIVE/training/mask/\"\n",
    "\n",
    "imgs_test = \"./DRIVE/test/images/\"\n",
    "label_imgs_test = \"./DRIVE/test/1st_manual/\"\n",
    "masks_test = \"./DRIVE/test/mask/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qUlmwxJXYQu8",
   "metadata": {
    "id": "qUlmwxJXYQu8"
   },
   "outputs": [],
   "source": [
    "image_paths = [imgs_train + str(i) + '_training.tif' for i in range(21, 41)]\n",
    "label_paths = [label_imgs_train + str(i) + '_manual1.gif' for i in range(21, 41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Oujute6UYQu8",
   "metadata": {
    "id": "Oujute6UYQu8"
   },
   "source": [
    "## Construct the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JP2govjCYQu8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JP2govjCYQu8",
    "outputId": "354048f4-1dd2-4166-852c-6f3b14421252"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer = len(image_paths) * (resize_wh // crop_wh)**2\n",
    "batch_size = 32\n",
    "\n",
    "print('Shuffle buffer size: {}'.format(shuffle_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OEtp2AxtYQu9",
   "metadata": {
    "id": "OEtp2AxtYQu9"
   },
   "outputs": [],
   "source": [
    "def load(image_paths, label_paths):\n",
    "    \n",
    "    image_string = tf.io.read_file(image_paths)\n",
    "    label_string = tf.io.read_file(label_paths)\n",
    "    \n",
    "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tfio.experimental.image.decode_tiff(image_string)\n",
    "\n",
    "    label = tf.image.decode_gif(label_string)\n",
    "    label = tf.squeeze(tf.image.rgb_to_grayscale(label), axis=0)\n",
    "    label_shape = label.shape    \n",
    "\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    \n",
    "    image.set_shape((height, width, channels))\n",
    "    label.set_shape((height, width, 1))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEsq4bQGYQu9",
   "metadata": {
    "id": "PEsq4bQGYQu9"
   },
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    ran = tf.random.uniform([2], minval=1 - 0.3, maxval=1 + 0.3)\n",
    "\n",
    "    image = tf.image.adjust_brightness(image, ran[0])\n",
    "    image = tf.image.adjust_contrast(image, ran[1])\n",
    "    \n",
    "    ran = tf.random.uniform([2], minval=1 - 0.02, maxval=1 + 0.02)\n",
    "\n",
    "    image = tf.image.adjust_saturation(image, ran[0])\n",
    "    image = tf.image.adjust_hue(image, ran[1])\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62MIa2-aYQu-",
   "metadata": {
    "id": "62MIa2-aYQu-"
   },
   "outputs": [],
   "source": [
    "class random_flip_rot(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.flip_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.flip_labels = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.rot_inputs = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "    self.rot_labels = tf.keras.layers.RandomRotation(0.15, seed=seed)\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    \n",
    "    ran = tf.random.uniform([2], maxval=1)\n",
    "\n",
    "    if ran[0] < 0.5:\n",
    "      inputs = self.flip_inputs(inputs)\n",
    "      labels = self.flip_labels(labels)\n",
    "\n",
    "    if ran[1] < 0.5:\n",
    "      inputs = self.rot_inputs(inputs)\n",
    "      labels = self.rot_labels(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqBgrvB4YQu-",
   "metadata": {
    "id": "kqBgrvB4YQu-"
   },
   "outputs": [],
   "source": [
    "def clahe_equalized(image):\n",
    "\n",
    "    image = image.numpy()\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image.astype(np.uint8))\n",
    "\n",
    "    return np.expand_dims(image, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9sLeYtLYQu_",
   "metadata": {
    "id": "i9sLeYtLYQu_"
   },
   "outputs": [],
   "source": [
    "def crop(image, label, cut_size, image_channels):\n",
    "\n",
    "  \"\"\"Returns a cropped square image.\"\"\"\n",
    "  \n",
    "  shape = image.shape\n",
    "\n",
    "  image_new = tf.zeros((0, cut_size, cut_size, image_channels))\n",
    "  label_new = tf.zeros((0, cut_size, cut_size, 1))\n",
    "\n",
    "\n",
    "  for idx in range(0, shape[1] // cut_size):\n",
    "    for idy in range(0, shape[0] // cut_size):\n",
    "\n",
    "      image_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          image, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "      label_aux = tf.expand_dims(tf.image.crop_to_bounding_box(\n",
    "          label, idy * cut_size, idx * cut_size, cut_size, cut_size), axis=0)\n",
    "    \n",
    "      image_new = tf.concat([image_new, image_aux], axis=0)\n",
    "      label_new = tf.concat([label_new, label_aux], axis=0)\n",
    "\n",
    "  return image_new, label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ODwmEcBzYQu_",
   "metadata": {
    "id": "ODwmEcBzYQu_"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pg-Zx6y3YQvA",
   "metadata": {
    "id": "pg-Zx6y3YQvA"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "\n",
    "dataset = dataset.map(load)\n",
    "dataset = dataset.map(augment)\n",
    "#dataset = dataset.map(random_flip_rot())\n",
    "\n",
    "dataset = dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.1)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(shuffle_buffer)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oQRe4Dl2YQvA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQRe4Dl2YQvA",
    "outputId": "272619e6-c514-4a9a-8305-0e89614ac44c"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zi75MldIZAB_",
   "metadata": {
    "id": "zi75MldIZAB_"
   },
   "source": [
    "## Define M2U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I4cQSqGWZHhd",
   "metadata": {
    "id": "I4cQSqGWZHhd"
   },
   "outputs": [],
   "source": [
    "save_path = 'my_M2Unet_DRIVE.h5'\n",
    "save_best_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_9c90C7ZQtV",
   "metadata": {
    "id": "T_9c90C7ZQtV"
   },
   "source": [
    "### Auxiliary functions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "esJWuNfNDqxY",
   "metadata": {
    "id": "esJWuNfNDqxY"
   },
   "outputs": [],
   "source": [
    "def DepthwiseConv(input, channels, stride=1):\n",
    "\n",
    "  x = tf.keras.layers.DepthwiseConv2D(kernel_size=3,\n",
    "                                      strides=stride,\n",
    "                                      padding='same')(input)\n",
    "\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)                                    \n",
    "  x = tf.keras.layers.Conv2D(channels, 1, strides=1, padding='same')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3t4yOn3RInUn",
   "metadata": {
    "id": "3t4yOn3RInUn"
   },
   "outputs": [],
   "source": [
    "def bottleneck_block(input, input_channels, output_channels, stride, factor):\n",
    "  \n",
    "  x = tf.keras.layers.Conv2D(round(input_channels * factor), 1, strides=1, padding='same')(input)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)  \n",
    "  x = DepthwiseConv(x, output_channels, stride)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5OWAWkzEMGBO",
   "metadata": {
    "id": "5OWAWkzEMGBO"
   },
   "outputs": [],
   "source": [
    "def res_bottle_neck(input, channels, factor):\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(round(channels * factor), 1, strides=1, padding='same')(input)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.activations.relu(x)\n",
    "  x = DepthwiseConv(x, channels, 1)\n",
    "  x = tf.keras.layers.Add()([x, input])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BqnWQYzNRI9s",
   "metadata": {
    "id": "BqnWQYzNRI9s"
   },
   "outputs": [],
   "source": [
    "def upconcat(input, skip_input):\n",
    "\n",
    "  x = tf.keras.layers.UpSampling2D(interpolation='bilinear')(input)\n",
    "  x = tf.concat([x, skip_input], axis=-1)  \n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wCo1PYo3ZW7A",
   "metadata": {
    "id": "wCo1PYo3ZW7A"
   },
   "source": [
    "## Build M2U-net\n",
    "\n",
    "- Build U-net with input (crop_hw, crop_hw, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wjU-HP5lDqvJ",
   "metadata": {
    "id": "wjU-HP5lDqvJ"
   },
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(input)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.activations.relu(x)\n",
    "\n",
    "skip1 = DepthwiseConv(x, channels=16, stride=1)\n",
    "\n",
    "x = bottleneck_block(skip1, 16, 24, 2, 6)\n",
    "skip2 = res_bottle_neck(x, 24, 6)\n",
    "\n",
    "x = bottleneck_block(skip2, 24, 32, 2, 6)\n",
    "x = res_bottle_neck(x, 32, 6)\n",
    "skip3 = res_bottle_neck(x, 32, 6)\n",
    "\n",
    "x = bottleneck_block(skip3, 32, 64, 2, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = res_bottle_neck(x, 64, 6)\n",
    "x = bottleneck_block(x, 64, 96, 1, 6)\n",
    "\n",
    "x = res_bottle_neck(x, 96, 6)\n",
    "x = res_bottle_neck(x, 96, 6)\n",
    "\n",
    "x = upconcat(x, skip3)\n",
    "x = bottleneck_block(x, 128, 64, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, skip2)\n",
    "x = bottleneck_block(x, 88, 44, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, skip1)\n",
    "x = bottleneck_block(x, 60, 30, 1, 0.15)\n",
    "\n",
    "x = upconcat(x, input)\n",
    "x = bottleneck_block(x, 30, 1, 1, 0.15)\n",
    "\n",
    "x = tf.keras.activations.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6iaBO2z3o0Ge",
   "metadata": {
    "id": "6iaBO2z3o0Ge"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=input, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec_dY-MpZfgF",
   "metadata": {
    "id": "ec_dY-MpZfgF"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HNPELNlcZfgF",
   "metadata": {
    "id": "HNPELNlcZfgF"
   },
   "source": [
    "### Dice and Jaccard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sF_zd_nTZfgG",
   "metadata": {
    "id": "sF_zd_nTZfgG"
   },
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "    \n",
    "  return jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy311ShvZfgG",
   "metadata": {
    "id": "xy311ShvZfgG"
   },
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    \n",
    "  y_true = tf.cast(y_true, tf.float32)\n",
    "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "  denominator = tf.reduce_sum(y_true + y_pred)\n",
    "\n",
    "  return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "We1r6hnsZfgH",
   "metadata": {
    "id": "We1r6hnsZfgH"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RnyaCeliZfgH",
   "metadata": {
    "id": "RnyaCeliZfgH"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(save_path,\n",
    "                               verbose=1, monitor='val_loss', mode='auto',\n",
    "                               save_best_only=save_best_only)\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, model, dataset, display_freq=9):\n",
    "\n",
    "    self.model = model\n",
    "    self.dataset = dataset\n",
    "    self.display_freq = display_freq\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "    if epoch % self.display_freq == 0:\n",
    "      \n",
    "      plt.figure(figsize=(10, 10))\n",
    "        \n",
    "      for data in self.dataset.take(1):\n",
    "\n",
    "          image = data[0].numpy()\n",
    "          label = data[1].numpy()\n",
    "\n",
    "          pred = self.model.predict(image)\n",
    "\n",
    "          print(' ')\n",
    "          \n",
    "          jac = jaccard(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "          dic = dice(data[1], tf.convert_to_tensor(pred)).numpy()\n",
    "\n",
    "          print('jaccard: {:2.2f}'.format(jac))\n",
    "          print('dice: {:2.2f}'.format(dic))\n",
    "\n",
    "          print(' ')\n",
    "\n",
    "          #pred[pred < 0.5] = 0\n",
    "          #pred[pred >= 0.5] = 1\n",
    "\n",
    "          ax = plt.subplot(2, 2, 1)\n",
    "            \n",
    "          plot = cv2.cvtColor(label[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('True')\n",
    "          plt.axis('off')\n",
    "          \n",
    "          ax = plt.subplot(2, 2, 2)\n",
    "            \n",
    "          plot = cv2.cvtColor(pred[0, ...].astype('float32'), cv2.IMREAD_COLOR)\n",
    "\n",
    "          plt.imshow(plot)\n",
    "          plt.title('Predicted')\n",
    "          plt.axis('off')\n",
    "\n",
    "      plt.show()\n",
    "    \n",
    "      auc = []\n",
    "\n",
    "      for data in self.dataset:\n",
    "            \n",
    "          pred = self.model.predict(data[0])\n",
    "          label = data[1].numpy()\n",
    "        \n",
    "          label[label > 0.5] = 1\n",
    "          label[label <= 0.5] = 0\n",
    "\n",
    "          auc.append(roc_auc_score(label.reshape(-1), pred.reshape(-1)))\n",
    "\n",
    "      auc = np.asarray(auc)\n",
    "\n",
    "      print(' ')\n",
    "      print('auc: {:2.2f}'.format(np.mean(auc), np.std(auc)))\n",
    "      print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S66SngtwZljs",
   "metadata": {
    "id": "S66SngtwZljs"
   },
   "source": [
    "## Compile and show the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbE-8JKgZljt",
   "metadata": {
    "id": "HbE-8JKgZljt"
   },
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  sum_ = tf.reduce_sum(y_true + y_pred)\n",
    "  jac = (intersection) / (sum_ - intersection)\n",
    "\n",
    "  jac = (1 - jac)\n",
    "\n",
    "  loss = (tf.keras.losses.binary_crossentropy(y_true, y_pred) +\n",
    "        0.3 * jac)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xqJHcAiOZlju",
   "metadata": {
    "id": "xqJHcAiOZlju"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=combined_loss, \n",
    "              metrics=['accuracy', jaccard, dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6vjaO-GZlju",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6vjaO-GZlju",
    "outputId": "8424e0c9-5951-4996-89ef-03450a96ac39"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R9xlGucUZvWF",
   "metadata": {
    "id": "R9xlGucUZvWF"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7jPUCW59ZvWF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7jPUCW59ZvWF",
    "outputId": "f8dce8f0-8bc6-4cf1-d86e-a286ca79f1e6"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=150, verbose=2, \n",
    "                    callbacks=[checkpointer, CustomCallback(model, train_dataset)],\n",
    "                    validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v8GkYAeiZvWG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v8GkYAeiZvWG",
    "outputId": "e99c3dcd-dae4-4465-edbc-703e4b6a3090"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    \n",
    "    ax = plt.subplot(8, 2, 1 + idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(history.history[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O00XEi6bQ90J",
   "metadata": {
    "id": "O00XEi6bQ90J"
   },
   "source": [
    "## Compare the results with the test images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054hedQQ90J",
   "metadata": {
    "id": "f054hedQQ90J"
   },
   "source": [
    "### Get the test dataset and evaluate the model in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FeOIgtqXQ90J",
   "metadata": {
    "id": "FeOIgtqXQ90J"
   },
   "outputs": [],
   "source": [
    "# The test batch size is equal to the number of pieces cut from the original\n",
    "# image, this was done so that we can reconstruct the image later.\n",
    "\n",
    "test_batch = (resize_wh // crop_wh)**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cuOLvoBqQ90J",
   "metadata": {
    "id": "cuOLvoBqQ90J"
   },
   "outputs": [],
   "source": [
    "def parse_function_test(image, label):\n",
    "\n",
    "    image = image[..., :3]\n",
    "    image_shape = image.shape\n",
    "    label_shape = label.shape\n",
    "\n",
    "    image = tf.image.resize(image, [resize_wh, resize_wh], method='nearest')\n",
    "    label = tf.image.resize(label, [resize_wh, resize_wh], method='nearest')\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) / 255\n",
    "    label = tf.image.convert_image_dtype(label, tf.float32) / 255\n",
    "\n",
    "    image, label = crop(image, label, crop_wh, image.shape[-1])\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MlEU-pnyQ90J",
   "metadata": {
    "id": "MlEU-pnyQ90J"
   },
   "outputs": [],
   "source": [
    "test_image_paths = [imgs_test + str(i).zfill(2) + '_test.tif' for i in range(1, 21)]\n",
    "test_label_paths = [label_imgs_test + str(i).zfill(2) + '_manual1.gif' for i in range(1, 21)]\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_label_paths))\n",
    "\n",
    "test_dataset = test_dataset.map(load)\n",
    "test_dataset = test_dataset.map(parse_function_test)\n",
    "\n",
    "test_dataset = test_dataset.flat_map(\n",
    "    lambda image, label: tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(image), \n",
    "    tf.data.Dataset.from_tensor_slices(label))\n",
    "    ))\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9gm3s_RuQ90J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gm3s_RuQ90J",
    "outputId": "01206d14-0278-4864-98ca-31ee634c8089"
   },
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eVsUliMlQ90J",
   "metadata": {
    "id": "eVsUliMlQ90J"
   },
   "source": [
    "### Predict and plot the segmented images\n",
    "\n",
    "- White: Ground Truth.\n",
    "- Blue: True Positive.\n",
    "- Green: False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MaCSNwkCQ90K",
   "metadata": {
    "id": "MaCSNwkCQ90K"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i7bS7flBQ90K",
   "metadata": {
    "id": "i7bS7flBQ90K"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(image_cropped, label_cropped, image_pred_cropped):\n",
    "    \n",
    "    image = np.ones((resize_wh, resize_wh, 3))\n",
    "    label = np.ones((resize_wh, resize_wh, 1))\n",
    "    image_pred = np.ones((resize_wh, resize_wh, 1))\n",
    "\n",
    "    cont = 0\n",
    "\n",
    "    for idx in range(0, image.shape[1] // crop_wh):\n",
    "        for idy in range(0, image.shape[0] // crop_wh):\n",
    "          \n",
    "          image[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_cropped[cont, ...]\n",
    "          label[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = label_cropped[cont, ...]\n",
    "          image_pred[idy * crop_wh: (idy + 1) * crop_wh, idx * crop_wh: (idx + 1) * crop_wh, :] = image_pred_cropped[cont, ...]\n",
    "          cont += 1\n",
    "\n",
    "    return image, label, image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VWLYmMs3Q90K",
   "metadata": {
    "id": "VWLYmMs3Q90K"
   },
   "outputs": [],
   "source": [
    "def color_labels(label, image_pred):\n",
    "\n",
    "    color_label = np.zeros((512, 512, 3))\n",
    "\n",
    "    i_t, j_t = np.where(label[..., 0] > 0.0)\n",
    "\n",
    "    # Color with white pixels where labels are equal to one.\n",
    "    color_label[i_t, j_t, :] = 255\n",
    "    \n",
    "    i, j = np.where(image_pred[..., 0] > 0.5)\n",
    "    \n",
    "    label_set = set([(a, b) for a, b in zip(i_t, j_t)])\n",
    "    pred_set = set([(a, b) for a, b in zip(i, j)])\n",
    "    pred_tp = set(label_set).intersection(pred_set)\n",
    "    \n",
    "    pred_fp = pred_set - pred_tp\n",
    "\n",
    "    k1, k2 = zip(*pred_tp)\n",
    "    color_label[k1, k2, 0:2] = 0\n",
    "\n",
    "    k1, k2 = zip(*pred_fp)\n",
    "    color_label[k1, k2, 1:2] = 255\n",
    "\n",
    "    return color_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BpIF9lliQ90K",
   "metadata": {
    "id": "BpIF9lliQ90K"
   },
   "outputs": [],
   "source": [
    "for data in test_dataset:\n",
    "  \n",
    "  image_cropped = data[0].numpy()\n",
    "  label_cropped = data[1].numpy()\n",
    "\n",
    "  image_pred_cropped = model.predict(image_cropped)\n",
    "\n",
    "  image, label, image_pred = rebuild_image(image_cropped,\n",
    "                                           label_cropped,\n",
    "                                           image_pred_cropped)\n",
    "  label = color_labels(label, image_pred)\n",
    "  \n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  ax = plt.subplot(1, 2, 1)\n",
    "  image = cv2.cvtColor(image.astype('float32'), cv2.IMREAD_COLOR)\n",
    " \n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  ax = plt.subplot(1, 2, 2)\n",
    "  pred_label = cv2.cvtColor(label.astype('uint8'), cv2.IMREAD_COLOR)\n",
    "\n",
    "  plt.imshow(pred_label)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e0IdaAX2rLWK",
    "apLtaT4XNmjy",
    "HzijGId66Mk6",
    "Vlam8K47FjFv",
    "S6natZyN6UNS",
    "LltVJjKgeAgg",
    "aEmGq6Yqe86R",
    "fxRpV0y2e_c-",
    "LGyq3M5J6Ith",
    "K4ySJByZ9mAZ",
    "Nh7BLGd_8_OR",
    "xHWJyspt9QKU",
    "L-_5QoZf9aIM",
    "y6-EccdRUEtK",
    "bc8d5XNArZne",
    "RB3xbLhYqozW",
    "a2Rf3zpMuuDF",
    "PqW-vto1uuDH",
    "4-VbZAS2v3eO",
    "Erz3P91-wKht",
    "2p7dmcL1wKht",
    "sxLXhFJtwKhu",
    "s-amCh2HwUML",
    "Lcgo3devwnYu",
    "GGNETOgEywQa",
    "aqv4hfmrywQa",
    "ToujTJ4TywQb",
    "QxRJOUd3DnlG",
    "2yrKfnnQYQu5",
    "Oujute6UYQu8",
    "zi75MldIZAB_",
    "T_9c90C7ZQtV",
    "wCo1PYo3ZW7A",
    "ec_dY-MpZfgF",
    "HNPELNlcZfgF",
    "We1r6hnsZfgH",
    "S66SngtwZljs",
    "R9xlGucUZvWF",
    "O00XEi6bQ90J",
    "f054hedQQ90J",
    "eVsUliMlQ90J"
   ],
   "name": "(Drive) U-net Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
